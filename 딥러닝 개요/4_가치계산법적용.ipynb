{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GXwVyT1u2iHT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "w46Xw4qj2kzK",
        "outputId": "912acde8-8156-4e40-d677-0929b566e6c0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 216x216 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 게임의 전체 크기\n",
        "fig = plt.figure( figsize=(3,3) )\n",
        "\n",
        "# 외곽 벽 그리기\n",
        "# 수평선 2개\n",
        "plt.plot( [0, 3],[ 3, 3], color='k') # 맨위 수평선\n",
        "plt.plot( [0, 3],[ 0, 0], color='k') # 맨아래 수평선\n",
        "plt.plot( [0, 0],[ 0, 2], color='k') # 왼쪽 수직선 -> 입구표시\n",
        "plt.plot( [3, 3],[ 1, 3], color='k') # 오른족 수직선\n",
        "\n",
        "# 내부 미로\n",
        "plt.plot( [1, 1],[1 , 2], color='k')\n",
        "plt.plot( [2, 3],[2 , 2], color='k')\n",
        "plt.plot( [1, 2],[1 , 1], color='k')\n",
        "plt.plot( [2, 2],[0 , 1], color='k')\n",
        "\n",
        "# 각 포인트 위치 정보 표시 -> 상태\n",
        "# 0,1,2 ~ 8까지 위치 정보 표시\n",
        "'''\n",
        "0 1 2\n",
        "3 4 5\n",
        "6 7 8\n",
        "'''\n",
        "for x in range(3):\n",
        "  for y in range(3):\n",
        "    plt.text( x + 0.5, 3 - y - 0.5, str(x+3*y), size=20, ha='center', va='center' )\n",
        "\n",
        "# 에이전트 표시 -> 시작위치 0번(0,0)에 위치\n",
        "mouse = plt.plot( 0 + 0.5, 3 - 0 - 0.5, marker='o', markersize=30, color='#dadada' )\n",
        "\n",
        "\n",
        "# 눈금정리\n",
        "plt.tick_params(bottom=False, left=False,          # - 표시 제거\n",
        "                labelbottom=False, labelleft=False # 수치값 제거\n",
        "                )\n",
        "\n",
        "# 차트 박스 제거\n",
        "plt.box( False )\n",
        "\n",
        "# 출력\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_OlDQU2V3LOZ"
      },
      "outputs": [],
      "source": [
        "# 게임이 시작하면 에이전트의 초기 위치\n",
        "AGENT_FIRST_STATE = 0\n",
        "# 에이전트 위치가 AGENT_LAST_STATE 와 같다면 게임 종료\n",
        "AGENT_LAST_STATE  = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPBAcCGD2lXp",
        "outputId": "d6044c71-7740-4ac9-cf84-17355a0e957b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[nan,  1.,  1., nan],\n",
              "       [nan,  1.,  1.,  1.],\n",
              "       [nan, nan, nan,  1.],\n",
              "       [ 1., nan,  1., nan],\n",
              "       [ 1.,  1., nan, nan],\n",
              "       [nan, nan,  1.,  1.],\n",
              "       [ 1.,  1., nan, nan],\n",
              "       [nan, nan, nan,  1.]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "theta_zero = np.array( [\n",
        "  # 상, 우, 하, 좌\n",
        "  [ np.nan, 1, 1, np.nan], #0\n",
        "  [ np.nan, 1, 1, 1], #1\n",
        "  [ np.nan, np.nan, np.nan, 1], #2\n",
        "  [ 1, np.nan, 1, np.nan], #3\n",
        "  [ 1, 1, np.nan, np.nan], #4\n",
        "  [ np.nan, np.nan, 1, 1], #5\n",
        "  [ 1, 1, np.nan, np.nan], #6\n",
        "  [ np.nan, np.nan, np.nan, 1], #7\n",
        "] )\n",
        "\n",
        "theta_zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7lVsjmk2q6S",
        "outputId": "69d008c5-3c93-4ae0-ca96-67d15c3e0fd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8, 4), array([[0.        , 0.5       , 0.5       , 0.        ],\n",
              "        [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
              "        [0.        , 0.        , 0.        , 1.        ],\n",
              "        [0.5       , 0.        , 0.5       , 0.        ],\n",
              "        [0.5       , 0.5       , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.5       , 0.5       ],\n",
              "        [0.5       , 0.5       , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 1.        ]]))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 직접 구현하는 활성화 함수 -> 확률로 계산해서 표현한다\n",
        "def mySoftmax( theta ):\n",
        "  # 결과를 담는 그릇\n",
        "  output = np.zeros_like( theta )\n",
        "\n",
        "  # 데이터가 큰수가 존재하고, 복잡한 범위를 가진 요소로 구성되어 있다면\n",
        "  # 지수함수로 일괄처리해서 진행\n",
        "  theta = np.exp( theta )\n",
        "\n",
        "  for i in range( theta.shape[0] ):\n",
        "    # 한줄씩 평균을 계산해서 대체\n",
        "    output[i] = theta[i] / np.nansum(theta[i])\n",
        "  \n",
        "  # nan -> 0 처리\n",
        "  return np.nan_to_num(output)\n",
        "\n",
        "policy = mySoftmax(theta_zero)\n",
        "policy.shape, policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gJxVSJTI2tXJ"
      },
      "outputs": [],
      "source": [
        "# 행동에 따른 다음 상태값 정보(포지션) 획득\n",
        "def getNextState( curState, nextAction ):\n",
        "  '''\n",
        "    curState   : 현재 상태\n",
        "    nextAction : 다음 행동\n",
        "  '''\n",
        "  if   nextAction == 0: # 위로 이동\n",
        "    return curState - 3\n",
        "  elif nextAction == 1: # 오른쪽 이동\n",
        "    return curState + 1\n",
        "  elif nextAction == 2: # 밑으로 이동\n",
        "    return curState + 3\n",
        "  elif nextAction == 3: # 왼쪽 이동\n",
        "    return curState - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRc0WWCR2w8w",
        "outputId": "a34d88f6-3920-45cd-ba34-c7efbe49b549"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[       nan, 0.00051245, 0.0093291 ,        nan],\n",
              "       [       nan, 0.00325322, 0.00574301, 0.00056701],\n",
              "       [       nan,        nan,        nan, 0.00406279],\n",
              "       [0.00127406,        nan, 0.00373417,        nan],\n",
              "       [0.00596914, 0.00738495,        nan,        nan],\n",
              "       [       nan,        nan, 0.00395818, 0.00104168],\n",
              "       [0.00888211, 0.00586321,        nan,        nan],\n",
              "       [       nan,        nan,        nan, 0.00353707]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x, y = theta_zero.shape\n",
        "Q    = np.random.rand( x, y ) * theta_zero * 0.01\n",
        "Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yhGgORSK2yOo"
      },
      "outputs": [],
      "source": [
        "# 특정 상태에서 특정 행동을 취하는 함수 새로 구성\n",
        "# 앱실론를 개입하여 적용\n",
        "def getAction( epsilon, policy, state, Q ):\n",
        "  if epsilon > np.random.rand(): # epsilon값이 만약 0.1이면 10% 확률로 \n",
        "    # 정책 경사법에서 사용한 선택방법을 활용하여 액션 결정\n",
        "    # 랜덤 행동(확률기반)\n",
        "    return np.random.choice([0,1,2,3], p=policy[state])\n",
        "  else:\n",
        "    # 상태값이 state일때 Q함수내에서 최대값을 가진 인덱스 선택\n",
        "    # 행동가치함수(Q함수 활용)\n",
        "    # nan을 제거한 수치들중에서 최대값의 인덱스\n",
        "    return np.nanargmax( Q[state] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2HKvlvDB20Mh"
      },
      "outputs": [],
      "source": [
        "def sarsa( Q, s, a, r, s_next, a_next ):      # Q함수가 갱신\n",
        "  '''\n",
        "    Q : 행동가치함수, 갱신의 대상, 정책기준\n",
        "    s : 에이전트의 현재 상태값(위치값)\n",
        "    s_next : 에이전트의 다음 상태값\n",
        "    a : 현재 상태 행동\n",
        "    a_next : 다음 상태 행동\n",
        "    r : 즉시 보상\n",
        "  '''\n",
        "  eta   = 0.1 # 학습률 고정, TD오차의 10%만 행동가치함수 갱신에 관여하겠다\n",
        "  gamma = 0.9 # 시간 할인율  -> 90% 만 적용(미래가치에) \n",
        "  # 에이전트의 위치가 마지막(골인)이다\n",
        "  if s_next == AGENT_LAST_STATE:\n",
        "    #Q[ s, a ] = Q[ s, a ] + eta * ( r + gamma*0 - Q[ s, a ] )\n",
        "    Q[ s, a ] = Q[ s, a ] + eta * ( r - Q[ s, a ] )\n",
        "    pass\n",
        "  else: #일반적인 모든 위치\n",
        "    Q[ s, a ] = Q[ s, a ] + eta * ( r + gamma*Q[ s_next, a_next ] - Q[ s, a ] )\n",
        "    pass\n",
        "  return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "snvv59zl21SQ"
      },
      "outputs": [],
      "source": [
        "def q_learning( Q, s, a, r, s_next, a_next ): # Q함수가 갱신\n",
        "  '''\n",
        "    Q : 행동가치함수, 갱신의 대상, 정책기준\n",
        "    s : 에이전트의 현재 상태값(위치값)\n",
        "    s_next : 에이전트의 다음 상태값\n",
        "    a : 현재 상태 행동\n",
        "    a_next : 다음 상태 행동\n",
        "    r : 즉시 보상\n",
        "  '''\n",
        "  eta   = 0.1 # 학습률 고정, TD오차의 10%만 행동가치함수 갱신에 관여하겠다\n",
        "  gamma = 0.9 # 시간 할인율  -> 90% 만 적용(미래가치에) \n",
        "  # 에이전트의 위치가 마지막(골인)이다\n",
        "  if s_next == AGENT_LAST_STATE:\n",
        "    Q[ s, a ] = Q[ s, a ] + eta * ( r - Q[ s, a ] )\n",
        "    pass\n",
        "  else: #일반적인 모든 위치\n",
        "    # 어떤 행동을 다음번 스텝에서 취해도 관계없다. 오직 값이 높은 행동만 선택\n",
        "    # 이것 때문에 수렴도 빠르지만 구소적일수 있다 \n",
        "    # nanmax() -> nan 빼고 값들 중에 최대값 획득\n",
        "    Q[ s, a ] = Q[ s, a ] + eta * ( r + gamma*np.nanmax( Q[ s_next, : ] ) - Q[ s, a ] )\n",
        "    pass\n",
        "\n",
        "  return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3eLEJPDm23Ew"
      },
      "outputs": [],
      "source": [
        "def game_play( epsilon, Q1, policy ):\n",
        "  # 초기 상태\n",
        "  s       = AGENT_FIRST_STATE\n",
        "  # 이동 로그\n",
        "  act_his = [ [ s, np.nan ] ]  \n",
        "  # 초기 다음 스텝 행동\n",
        "  a_next  = getAction( epsilon, policy, s, Q1 )  \n",
        "  # 반복 작업 -> 에이전트가 8번 위치에 도착할때까지\n",
        "  while True:\n",
        "    # 1. 다음 행동을 현재 행동으로 변경\n",
        "    a = a_next    \n",
        "    # 2. 현재 상태에서 취할 행동으로 인한 다음 상태값 획득\n",
        "    s_next = getNextState( s, a )\n",
        "    #print( s, a, s_next  )\n",
        "    # 3. 로그 한개 완성 -> 값 수정 : np.nan => 행동값으로 대체\n",
        "    act_his[-1][-1] = a    \n",
        "    # 4. 새로운 로그 추가\n",
        "    act_his.append( [ s_next, np.nan ] )\n",
        "    # 5. 가치 계산을 위한 보상체크\n",
        "    if AGENT_LAST_STATE == s_next:\n",
        "      r = 1           # 다음 상태가 골인 지점이면 즉시보상이 1 제공\n",
        "      a_next = np.nan #  다음 행동을 없다 (게임종료)\n",
        "    else:\n",
        "      r      = 0      # 이외의 상태는 전부 즉시보상이 0\n",
        "      a_next = getAction( epsilon, policy, s_next, Q1 )\n",
        "    \n",
        "    # 6. 행동가치함수 갱신 => Q 갱신 => sarsa, q_learnning\n",
        "    #    1회 행동 => 해당 포지션에 대한 q함수 갱신    \n",
        "    #Q1 = sarsa( Q1, s, a, r, s_next, a_next )   \n",
        "    Q1 = q_learning( Q, s, a, r, s_next, a_next )\n",
        "    \n",
        "    # 에이전트의 다음 상태값이 최종위치(8)이면 종료\n",
        "    if AGENT_LAST_STATE == s_next:\n",
        "      break\n",
        "    else:\n",
        "      # 에이전트의 다음 상태값이 현재 상태값으로 치환\n",
        "      s = s_next\n",
        "\n",
        "  return act_his, Q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWTGbxks4lyo",
        "outputId": "8746d17e-d954-4c80-aa28-59d1ec31dc6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[       nan, 0.00697983, 0.0033506 ,        nan],\n",
              "       [       nan, 0.00157739, 0.00315892, 0.00109172],\n",
              "       [       nan,        nan,        nan, 0.00612643],\n",
              "       [0.00439977,        nan, 0.00137643,        nan],\n",
              "       [0.00356837, 0.00362323,        nan,        nan],\n",
              "       [       nan,        nan, 0.00355995, 0.0032448 ],\n",
              "       [0.0058517 , 0.00323553,        nan,        nan],\n",
              "       [       nan,        nan,        nan, 0.00734546]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x, y = theta_zero.shape\n",
        "Q    = np.random.rand( x, y ) * theta_zero * 0.01\n",
        "Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aofAAjr54odB",
        "outputId": "3947331e-7e73-456b-844d-1b53cd67cec8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 0.5       , 0.5       , 0.        ],\n",
              "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
              "       [0.        , 0.        , 0.        , 1.        ],\n",
              "       [0.5       , 0.        , 0.5       , 0.        ],\n",
              "       [0.5       , 0.5       , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.5       , 0.5       ],\n",
              "       [0.5       , 0.5       , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 1.        ]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XyzLm1u24oh",
        "outputId": "90c604ff-d948-43cc-a219-a52cc4d73dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[       nan 0.00697983 0.0033506         nan]\n",
            " [       nan 0.00157739 0.00315892 0.00109172]\n",
            " [       nan        nan        nan 0.00612643]\n",
            " [0.00439977        nan 0.00137643        nan]\n",
            " [0.00356837 0.00362323        nan        nan]\n",
            " [       nan        nan 0.00355995 0.0032448 ]\n",
            " [0.0058517  0.00323553        nan        nan]\n",
            " [       nan        nan        nan 0.00734546]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[       nan, 0.01358032, 0.0033506 ,        nan],\n",
              "       [       nan, 0.00197103, 0.05977794, 0.00109172],\n",
              "       [       nan,        nan,        nan, 0.00579809],\n",
              "       [0.00439977,        nan, 0.00137643,        nan],\n",
              "       [0.00356837, 0.24001558,        nan,        nan],\n",
              "       [       nan,        nan, 0.65256284, 0.0032448 ],\n",
              "       [0.0058517 , 0.00323553,        nan,        nan],\n",
              "       [       nan,        nan,        nan, 0.00734546]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# 시뮬레이션\n",
        "epsilon = 0.5 # 50 확률로 램덤으로 선택됨 -> 계속 50%씩 수치를 줄이겟다\n",
        "\n",
        "print( Q )\n",
        "for epi in range( 10 ): # 10회만 게임 진행\n",
        "  # 1. 에피소드 진행 : 에이전트 이동 로그,  갱신된 Q함수가 리턴\n",
        "  act_his, Q = game_play( epsilon, Q, policy )\n",
        "  # 2. 로그\n",
        "  #print( '에피소드 {0:2} 에이전트 이동수 {1:3} 엡신론 {2}'.format(epi+1, len(act_his), epsilon) )\n",
        "  # 3. 엡실론 값을 계속 에피소드가 전개 될때마다 50% 감소\n",
        "  epsilon /= 2\n",
        "\n",
        "Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "0JMKCWHd3A2H",
        "outputId": "8482d256-402d-48a2-8362-1530a6c989df"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nsarsa 로그\\n에피소드  1 에이전트 이동수  15 엡신론 0.5\\n에피소드  2 에이전트 이동수  21 엡신론 0.25\\n에피소드  3 에이전트 이동수  25 엡신론 0.125\\n에피소드  4 에이전트 이동수   5 엡신론 0.0625\\n에피소드  5 에이전트 이동수   5 엡신론 0.03125\\n에피소드  6 에이전트 이동수   5 엡신론 0.015625\\n에피소드  7 에이전트 이동수   5 엡신론 0.0078125\\n에피소드  8 에이전트 이동수   5 엡신론 0.00390625\\n에피소드  9 에이전트 이동수   5 엡신론 0.001953125\\n에피소드 10 에이전트 이동수   5 엡신론 0.0009765625\\n\\nQ Learning 로그\\n에피소드  1 에이전트 이동수  21 엡신론 0.5\\n에피소드  2 에이전트 이동수  81 엡신론 0.25\\n에피소드  3 에이전트 이동수  21 엡신론 0.125\\n에피소드  4 에이전트 이동수   5 엡신론 0.0625\\n에피소드  5 에이전트 이동수   5 엡신론 0.03125\\n에피소드  6 에이전트 이동수   5 엡신론 0.015625\\n에피소드  7 에이전트 이동수   5 엡신론 0.0078125\\n에피소드  8 에이전트 이동수   5 엡신론 0.00390625\\n에피소드  9 에이전트 이동수   5 엡신론 0.001953125\\n에피소드 10 에이전트 이동수   5 엡신론 0.0009765625\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "sarsa 로그\n",
        "에피소드  1 에이전트 이동수  15 엡신론 0.5\n",
        "에피소드  2 에이전트 이동수  21 엡신론 0.25\n",
        "에피소드  3 에이전트 이동수  25 엡신론 0.125\n",
        "에피소드  4 에이전트 이동수   5 엡신론 0.0625\n",
        "에피소드  5 에이전트 이동수   5 엡신론 0.03125\n",
        "에피소드  6 에이전트 이동수   5 엡신론 0.015625\n",
        "에피소드  7 에이전트 이동수   5 엡신론 0.0078125\n",
        "에피소드  8 에이전트 이동수   5 엡신론 0.00390625\n",
        "에피소드  9 에이전트 이동수   5 엡신론 0.001953125\n",
        "에피소드 10 에이전트 이동수   5 엡신론 0.0009765625\n",
        "\n",
        "Q Learning 로그\n",
        "에피소드  1 에이전트 이동수  21 엡신론 0.5\n",
        "에피소드  2 에이전트 이동수  81 엡신론 0.25\n",
        "에피소드  3 에이전트 이동수  21 엡신론 0.125\n",
        "에피소드  4 에이전트 이동수   5 엡신론 0.0625\n",
        "에피소드  5 에이전트 이동수   5 엡신론 0.03125\n",
        "에피소드  6 에이전트 이동수   5 엡신론 0.015625\n",
        "에피소드  7 에이전트 이동수   5 엡신론 0.0078125\n",
        "에피소드  8 에이전트 이동수   5 엡신론 0.00390625\n",
        "에피소드  9 에이전트 이동수   5 엡신론 0.001953125\n",
        "에피소드 10 에이전트 이동수   5 엡신론 0.0009765625\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XusgryWl_kTK"
      },
      "source": [
        "# 결론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47PJmCAD_l6R"
      },
      "source": [
        "- 게임이 복잡해 질수록 Q 함수도 같이 커지고, 이를 처리하는 연산량도 같이 증가 문제 => 비효율적이 된다\n",
        "- 복잡한 문제는 딥러닝을 활용하여 처리한다\n",
        "  - DQN은 딥러닝 배우고 나서 진입, CNN 학습 이후 다시 체크"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "4.가치계산법적용.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
