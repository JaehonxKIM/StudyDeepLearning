# -*- coding: utf-8 -*-
"""4. 텐서플로우 1.x를 이용한 CNN 구현및 MINIST 손글씨 이미지 분류_최종_텐서보드구현.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fklCSkCZhH4hKHHZqmbqivY7j07zV1c6

- 전 장에서 에러가 나 코드 수정
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 텐서플로우 엔진을 기반한 딥러닝 개발방법

- 저수준 API로 개발
  - tensorflow api를 이용해 개발
    - 1.x
    - 2.x
  - 내부를 직접 설계할 수 있다
- 고수준 API로 개발
  - KERAS를 이용해 개발
  - 제공된 틀에서만 개발 가능하다
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
tf.__version__

# 코랩용 텐서보드 설치
!pip install tensorboardColab

from tensorboardcolab import *
import os, shutil

# 텐서보드에 필요한 데이터 저장 공간 생성, 필요시 이전 기록 삭제
try:
  if os.path.exists('./Graph'):# 존재하면
    shutil.rmtree('./Graph', ignore_errors=True)
  # 폴더 생성
  os.mkdir('./Graph')
except Exception as e:
  print( e )

# 텐서보드 생성
tbc = TensorBoardColab()
# 오남용사례가 많아서 구글에서 블럭 요청 처리되어 이제 사용 불가

import numpy as np
from tensorflow.examples.tutorials.mnist import input_data

# 데이터 획득
mnist     = input_data.read_data_sets('./data/mnist', one_hot=True)

# 환경변수
PIXEL     = mnist.train.images.shape[1]
PIXEL_H   = int( np.sqrt( PIXEL ) )
PIXEL_W   = PIXEL_H
LABEL_NUM = mnist.train.labels.shape[-1]

x = tf.placeholder( tf.float32, shape=( None, PIXEL ), name='x' )

def createFilterByWeight( name, shape ):
  name = f'{name}_W'
  initial_value = tf.truncated_normal( shape, stddev=0.1 )
  W = tf.Variable( initial_value=initial_value,  name=name )
  return W

def createBias( name, shape, value ):  
  name          = f'{name}_b'  
  initial_value = tf.constant( value, shape=shape )  
  b = tf.Variable( initial_value=initial_value, name=name)
  return b 

def createConv2D( name, x, W ):    
  name = f'{name}_conv'  
  return tf.nn.conv2d( x, filter=W, strides=[1,1,1,1], padding="SAME", name=name )

# 합성곱층 1층 생성
with tf.name_scope( '1f_conv' ) as scope:
  conv_1f_W   = createFilterByWeight( '1f_conv', (5, 5, 1, 32) )
  conv_1f_b   = createBias( '1f_conv', shape=(32,), value=0.1  )
  x_4d        = tf.reshape( x, ( -1, PIXEL_H, PIXEL_W, 1 ) )
  conv_1f     = createConv2D( '1f', x_4d, conv_1f_W ) + conv_1f_b
  act_conv_1f = tf.nn.relu( conv_1f )

def createMaxPooling( name, x ):  
  return tf.nn.max_pool( x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name=f'{name}_max')

# 풀링 1층 생성
with tf.name_scope( 'pooling_1f' ) as scope:  
  pool_1f = createMaxPooling( 'pooling_1f',  act_conv_1f)

# 합성곱 2층 생성
with tf.name_scope( '2f_conv' ) as scope:
  conv_2f_W   = createFilterByWeight( '2f_conv', (5, 5, 32, 32*2) )
  conv_2f_b   = createBias( '2f_conv', shape=(32*2,), value=0.1  )
  conv_2f     = createConv2D( '2f', pool_1f, conv_2f_W ) + conv_2f_b
  act_conv_2f = tf.nn.relu( conv_2f )

# 풀링 2층 생성
with tf.name_scope( 'pooling_2f' ) as scope:  
  pool_2f = createMaxPooling( 'pooling_2f',  act_conv_2f)

# 전결합층 생성 : Flattern
with tf.name_scope( 'fc' ) as scope:  
  _, h, w, ch  = pool_2f.shape
  in_channels  = h * w * ch     # 784 => 7*7*64 => 3306
  out_channels = 1024           # 3306 -> 1024 -> 10
  fc_x   = tf.reshape( pool_2f, ( -1, in_channels ) )   # 4D -> 2D
  fc_W   = createFilterByWeight( 'fc', (in_channels, out_channels) ) # (?, 3306)*(3306, 1024) => (?, 1024)
  fc_b   = createBias( 'fc', (out_channels, ), 0.1 )
  fc     = tf.matmul( fc_x , fc_W) + fc_b
  act_fc = tf.nn.relu( fc )

# 드롭아웃층 -> 과적합 방지를 위해 학습을 방해 -> 신경망을 특정비율로 죽인다
with tf.name_scope( 'act_fc_dropout' ) as scope:
  keep_prob      = tf.placeholder( tf.float32 )
  act_fc_dropout = tf.nn.dropout( act_fc, rate=1-keep_prob)

# 출력층 생성
with tf.name_scope( 'output' ) as scope:
  _, in_ch = act_fc_dropout.shape
  y_W      = createFilterByWeight( 'ouptut', ( in_ch, LABEL_NUM ) )  # (?,1024)*(1024, 10) => (?,10)
  y_b      = createBias( 'ouptut', (LABEL_NUM, ), 0.1 )
  y_conv   = tf.matmul( act_fc_dropout, y_W ) + y_b
  y_conv   = tf.nn.softmax( y_conv )

# 실제 정답
y_ = tf.placeholder( tf.float32, shape=(None, LABEL_NUM), name='y_')

# 손실함수 
with tf.name_scope( 'loss' ) as scope:
  cross_entropy = -tf.reduce_sum( y_ * tf.log( y_conv ) )

# 최적화
with tf.name_scope( 'adam' ) as scope:
  optimizer = tf.train.AdamOptimizer()
  # 훈련도구 생성
  train     = optimizer.minimize( cross_entropy )

  # 예측
with tf.name_scope( 'predict' ) as scope:
  predict   = tf.equal( tf.arg_max( y_conv, 1 ), tf.arg_max( y_, 1 ) )
  # 정확도
  accuracy  = tf.reduce_mean( tf.cast( predict, tf.float32 ) )

# 데이터 주입용 함수
def createFeedDict( x_train_data, y_train_label, prob ):
  return {
      x:x_train_data, 
      y_:y_train_label,
      keep_prob:prob
  }

# 학습
losses = list()
with tf.device('/device:GPU:0'):
  with tf.Session() as sess:  
    TRAIN_TOTAL_COUNT = 3000
    BATCH_SIZE        = 50  
    VERBOSE_INTERVAL  = 100     
    sess.run( tf.global_variables_initializer() )  
    test_fd = createFeedDict( mnist.test.images, mnist.test.labels, 1.0 )
    for step in range( TRAIN_TOTAL_COUNT ):    
      x_batch      = mnist.train.next_batch( BATCH_SIZE )    
      train_fd     = createFeedDict( x_batch[0], x_batch[1], 0.1 )   
      acc, _, loss = sess.run( [ accuracy, train, cross_entropy ], feed_dict=train_fd )
      #losses.append( loss )    
      if step % VERBOSE_INTERVAL == 0:    
        acc     = sess.run( accuracy, feed_dict=test_fd )
        print( f'step={step:4} acc={acc:20}, loss={loss:20}')
      pass    
    acc     = sess.run( accuracy, feed_dict=test_fd )
    print( f'step={step:4} acc={acc:20}, loss={loss:20}')  

    # 텐서보드 데이터 기록
    writer = tbc.get_writer()
    writer.add_graph(sess.graph)
    writer.flush()
# 텐서보드 코랩 닫기
tbc.close()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir ./Graph

